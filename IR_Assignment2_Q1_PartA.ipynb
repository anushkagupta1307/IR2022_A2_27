{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment2_Q1_PartA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushkagupta1307/IR2022_A2_27/blob/main/IR_Assignment2_Q1_PartA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 - Jaccard Coefficient**"
      ],
      "metadata": {
        "id": "CHc94_SYX6Qv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tREy8kVL9f-6",
        "outputId": "b25f1d06-7a4c-41b2-dbe3-c5cddf5499c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mount the data files from google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "import os\n",
        "import pydrive\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "#nltk library for preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "E4c8g-Nb9lGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db857c8-f8a0-4b1e-f03e-da66f5227717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This method takes the Data files as input, does preprocessing steps on the data and then creates a unigram inverted index\n",
        "\n",
        "def preprocessing_indexing():\n",
        "  #dictionary contains docID as the key and then have the all the tokens present in it as the value\n",
        "  token_dict=dict()\n",
        "  #doc-docID mapping done with docID as key and docName as value\n",
        "  doc_docID_mapping=dict()\n",
        "\n",
        "  directory = '/content/drive/MyDrive/Humor,Hist,Media,Food'\n",
        "\n",
        "  docID=1\n",
        "  for filename in os.scandir(directory):\n",
        "      flag=0\n",
        "      if filename.is_file():\n",
        "          ccfile = open(filename.path, \"r\",encoding=\"utf8\", errors=\"surrogateescape\")\n",
        "          doc_docID_mapping[docID]=filename.path\n",
        "\n",
        "          for aline in ccfile:\n",
        "              #Preprocessing steps included- \n",
        "              # Convert to lowercase\n",
        "              aline = aline.lower()\n",
        "              # Remove numerics\n",
        "              aline = re.sub(r'\\d+','',aline)\n",
        "              # Remove whitespaces\n",
        "              aline = re.sub(r'[^\\w\\s]','', aline)\n",
        "              # Remove special characters\n",
        "              aline= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',aline)\n",
        "              aline = aline.strip()\n",
        "              #Remove stopwords\n",
        "              aline = remove_stopwords(aline)\n",
        "              #Convert to tokens\n",
        "              words = word_tokenize(aline)\n",
        "          \n",
        "              #Create a dictionary for the index - the dictionary has the Root word(Stemming) as key and list of docIds as the value\n",
        "              for w in words:\n",
        "                if docID not in token_dict:\n",
        "                  list1=[ps.stem(w)]\n",
        "                  token_dict[docID]=(list1)\n",
        "                else:\n",
        "                  token_dict[docID].append(ps.stem(w))\n",
        "\n",
        "          docID=docID+1              \n",
        "\n",
        "\n",
        "  return token_dict,doc_docID_mapping"
      ],
      "metadata": {
        "id": "tSlshQ_M9w8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removed duplicate tokens from the dictionary values\n",
        "token_dict,doc_docID_mapping=preprocessing_indexing()\n",
        "token_dict_final=dict()\n",
        "for key, value in token_dict.items():\n",
        "    temp_set=set(value)\n",
        "    token_dict_final[key]=list(temp_set)\n"
      ],
      "metadata": {
        "id": "oZtcQSFgKLn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spread in greased"
      ],
      "metadata": {
        "id": "2w4wlh4ZLR0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding intersection of two lists\n",
        "def intersection_of_two_sets(a,b):\n",
        "    a_set = set(a)\n",
        "    b_set = set(b)\n",
        "    count=0.0\n",
        "    if (a_set & b_set):\n",
        "        count=count+1\n",
        "    \n",
        "    return count\n",
        "\n",
        "\n",
        "\n",
        "def jaccards_top_five_docs():\n",
        "\n",
        "  query=input(\"Please enter the query ::  \")\n",
        "  query=query.split()\n",
        "  list_words=[]\n",
        "\n",
        "  #pre-processed the input query\n",
        "  for word in query:\n",
        "        word = word.lower()\n",
        "        word = re.sub(r'\\d+','',word)\n",
        "        word = re.sub(r'[^\\w\\s]','', word)\n",
        "        word= re.sub('[@_!#$%^&*()<>?/\\|}{~:]','',word)\n",
        "        word = word.strip()\n",
        "        word = remove_stopwords(word)\n",
        "\n",
        "        if word!='':\n",
        "            list_words.append(word)\n",
        "\n",
        "  #found jaccards coefficient for each doc and stored result in the dictionary\n",
        "  jaccards_coeff_dict=dict()\n",
        "  for key, value in token_dict_final.items():\n",
        "      doc_list=value\n",
        "      intersection_doc_query=intersection_of_two_sets(list_words,doc_list)\n",
        "      union_doc_query=(len(list_words)+len(doc_list)-(intersection_doc_query))\n",
        "      jaccards_coeff_dict[key]=float(float(intersection_doc_query)/float(union_doc_query))\n",
        "\n",
        "  \n",
        "  #finding top five docs according to Jacard's Coefficient\n",
        "  top_five_docs=[]\n",
        "  for i in range(5):\n",
        "    maxval = max(jaccards_coeff_dict.values())\n",
        "    res = [(k, v) for k, v in jaccards_coeff_dict.items() if v == maxval]\n",
        "    del[jaccards_coeff_dict[res[0][0]]]\n",
        "    top_five_docs.append(res)\n",
        "\n",
        "\n",
        "  if (len(top_five_docs))>=5:\n",
        "    print(\"Found the following top five documents ranked according to the Jaccard's Coefficient Value :: \")\n",
        "    print()\n",
        "\n",
        "    for i in range(len(top_five_docs)):\n",
        "      rank=str(i+1)\n",
        "      print(\"details about the \"+rank+\" ranked document is given as follows\")\n",
        "      print(\"name of the document is :: \" + doc_docID_mapping[top_five_docs[i][0][0]])\n",
        "      jaccards_value_at_doc=str(top_five_docs[i][0][1])\n",
        "      print(\"Jaccard's Coefficient for the respective doc is :: \"+jaccards_value_at_doc)\n",
        "      print(\"--------------\")\n",
        "      print()\n",
        "  else:\n",
        "    print(\"Found the following top \"+len(top_five_docs)+\" ranked documnets according to the Jaccard's Coefficient Value :: \")\n",
        "\n",
        "    for i in range(len(top_five_docs)):\n",
        "      rank=str(i+1)\n",
        "      print(\"details about the \"+rank+\" ranked document is given as follows\")\n",
        "      print(\"name of the document is :: \" + doc_docID_mapping[top_five_docs[i][0][0]])\n",
        "      jaccards_value_at_doc=str(top_five_docs[i][0][1])\n",
        "      print(\"Jaccard's Coefficient for the respective doc is :: \"+jaccards_value_at_doc)\n",
        "      print(\"--------------\")\n",
        "      print()\n",
        "\n",
        "jaccards_top_five_docs()\n",
        "     \n",
        "  \n"
      ],
      "metadata": {
        "id": "Sbz3NG6nZCU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ce5836-d783-44b5-83cf-1b80b6a85da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the query ::  had finished forth a challenge\n",
            "Found the following top five documents ranked according to the Jaccard's Coefficient Value :: \n",
            "\n",
            "details about the 1 ranked document is given as follows\n",
            "name of the document is :: /content/drive/MyDrive/Humor,Hist,Media,Food/languag.jok\n",
            "Jaccard's Coefficient for the respective doc is :: 0.018518518518518517\n",
            "--------------\n",
            "\n",
            "details about the 2 ranked document is given as follows\n",
            "name of the document is :: /content/drive/MyDrive/Humor,Hist,Media,Food/cockney.alp\n",
            "Jaccard's Coefficient for the respective doc is :: 0.009523809523809525\n",
            "--------------\n",
            "\n",
            "details about the 3 ranked document is given as follows\n",
            "name of the document is :: /content/drive/MyDrive/Humor,Hist,Media,Food/policpig.hum\n",
            "Jaccard's Coefficient for the respective doc is :: 0.008130081300813009\n",
            "--------------\n",
            "\n",
            "details about the 4 ranked document is given as follows\n",
            "name of the document is :: /content/drive/MyDrive/Humor,Hist,Media,Food/parabl.hum\n",
            "Jaccard's Coefficient for the respective doc is :: 0.007575757575757576\n",
            "--------------\n",
            "\n",
            "details about the 5 ranked document is given as follows\n",
            "name of the document is :: /content/drive/MyDrive/Humor,Hist,Media,Food/curiousgeorgie.txt\n",
            "Jaccard's Coefficient for the respective doc is :: 0.006711409395973154\n",
            "--------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6VaX6d6oR-TV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}