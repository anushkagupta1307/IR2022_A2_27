{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment2_Q3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oa4UZQUFUQG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db6aba8-d584-4800-f2f1-822f21961eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import string\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.collections import defaultdict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UncHOA_Uo3F",
        "outputId": "4c3d2119-97fc-486d-852b-a601c8cdb0ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the text data....\n",
        "def preprocess(data):\n",
        "  \n",
        "  # converting to lowercase...\n",
        "  data = data.lower()\n",
        "\n",
        "  # remove tags...\n",
        "  remove_tag= re.compile(r'<[^>]+>')\n",
        "  data = remove_tag.sub(' ',data)\n",
        "\n",
        "  #punctuations and digits removal..\n",
        "  punc_dig_list = string.punctuation + string.digits\n",
        "  data = ' '.join(data.translate(str.maketrans(punc_dig_list , ' '*len(punc_dig_list ))).split())\n",
        "\n",
        "  # removing URL's...\n",
        "  data = re.sub(r'http\\S+', ' ',data)\n",
        "\n",
        "  # removing non alphanumeric characters\n",
        "  token = word_tokenize(data)\n",
        "  text = \"\"\n",
        "  text = text + ' '.join([word for word in token if word.isalnum()])\n",
        "  data = text\n",
        "  \n",
        "  # lemmatization..\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  token = word_tokenize(data)\n",
        "  text = \"\"\n",
        "  text = text + ' '.join([lemmatizer.lemmatize(word) for word in token ])\n",
        "  data = text\n",
        "\n",
        "  # removing stopwords and single,double letter words ..\n",
        "  text = \"\"\n",
        "  stopword = nltk.corpus.stopwords.words('english')\n",
        "  token = word_tokenize(data)\n",
        "  text = text + ' '.join([word for word in token if word not in stopword and len(word)>2])\n",
        "  data = text\n",
        "\n",
        "  # removing unwanted spaces..\n",
        "  data = re.sub(' +', ' ', data)\n",
        "\n",
        "  data = word_tokenize(data)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "ApkC6-mcUo9b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the document dataset into train,test..\n",
        "def split(dataset,split_size):\n",
        "  permuted_dataset = dataset.sample(frac=1)\n",
        "  train_size = int(len(permuted_dataset) * split_size)\n",
        "  train_set = permuted_dataset[:train_size]\n",
        "  test_set = permuted_dataset[train_size:]\n",
        "  return train_set['Documents'].tolist(),test_set['Documents'].tolist()"
      ],
      "metadata": {
        "id": "xWO8_DYUUuwt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create a dataframe of docs from the path pased as input...\n",
        "def get_dataframe(path):\n",
        "  docs=[]\n",
        "  for doc in os.listdir(path):\n",
        "    docs.append(doc)\n",
        "  df = pd.DataFrame(docs,columns=['Documents'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "P6qZWVhmUwVh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that will calculate Term Frequency of each word in each set of documents in the training set, given as input..\n",
        "# returns the dictionary of unique words with their TF found..\n",
        "def findTF(path,train_set):\n",
        "  vocab={}  \n",
        "  for docname in glob.glob(os.path.join(path,'*')):\n",
        "      if(os.path.basename(docname) in train_set):\n",
        "        with open(docname,'r',encoding=\"utf8\",errors='ignore') as f:\n",
        "          doc_text = f.read()\n",
        "          processed_list = preprocess(doc_text)\n",
        "          for word in processed_list:\n",
        "            if(word not in vocab):\n",
        "                vocab[word]=1\n",
        "            else:\n",
        "                vocab[word] = vocab[word] + 1\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "FpOlIIW9Ux5V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate the TF-ICF for each word in the dictionary passed as input....\n",
        "def calculate_TFICF(dictionary,vocab2,vocab3,vocab4,vocab5):\n",
        "  tficf_dict={}\n",
        "  for word in dictionary:\n",
        "    TF = dictionary[word]\n",
        "    CF = 1\n",
        "    if(word in vocab2):\n",
        "      CF = CF + 1 \n",
        "    if(word in vocab3):\n",
        "      CF = CF + 1 \n",
        "    if(word in vocab4):\n",
        "      CF = CF + 1 \n",
        "    if(word in vocab5):\n",
        "      CF = CF + 1 \n",
        "    ICF = math.log(5/CF)\n",
        "    TF_ICF = TF * ICF\n",
        "    tficf_dict[word] = TF_ICF\n",
        "\n",
        "  return tficf_dict"
      ],
      "metadata": {
        "id": "ZfjpI8PNUzca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create the effective vocabulaty containing top k features of each class...\n",
        "def effective_Vocab(dict1,dict2,dict3,dict4,dict5,k):\n",
        "  vocab=[]\n",
        "  for word in (list(dict1.keys())[:k]):\n",
        "    if(word not in vocab):\n",
        "      vocab.append(word)\n",
        "  for word in (list(dict2.keys())[:k]):\n",
        "    if(word not in vocab):\n",
        "      vocab.append(word)\n",
        "  for word in (list(dict3.keys())[:k]):\n",
        "    if(word not in vocab):\n",
        "      vocab.append(word)\n",
        "  for word in (list(dict4.keys())[:k]):\n",
        "    if(word not in vocab):\n",
        "      vocab.append(word)\n",
        "  for word in (list(dict5.keys())[:k]):\n",
        "    if(word not in vocab):\n",
        "      vocab.append(word)\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "_nywXlICU1RS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to find the word occurance of the top k features in each document in the path and train_set provided as input...\n",
        "def get_wordOccurance(path,train_set,effective_features):\n",
        "  word_count=[]\n",
        "  for docname in glob.glob(os.path.join(path,'*')):\n",
        "      if(os.path.basename(docname) in train_set):\n",
        "        with open(docname,'r',encoding=\"utf8\",errors='ignore') as f:\n",
        "          doc_text = f.read()\n",
        "          processed_list = preprocess(doc_text)\n",
        "          temp=[]\n",
        "          for word in effective_features:\n",
        "              if(word in processed_list):\n",
        "                temp.append(1)\n",
        "              else:\n",
        "                temp.append(0)\n",
        "          word_count.append(temp)\n",
        "  return(word_count)"
      ],
      "metadata": {
        "id": "V-o3HqVgU8LL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to compute the accuracy..\n",
        "def find_accuracy(y_true,y_pred):\n",
        "  count = 0\n",
        "  for i in range(0,len(y_true)):\n",
        "      if(y_true[i] == y_pred[i]):\n",
        "        count = count + 1\n",
        "  return (count/len(y_true))"
      ],
      "metadata": {
        "id": "Twyieb1CCrQJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function implementing Naive Bayes Algorithm..\n",
        "# Returns the predictions..\n",
        "def predict_NB(train_data,test_data,effective_features,train_split_ratio):\n",
        "\n",
        "  train_size = int(train_split_ratio * 1000)\n",
        "\n",
        "  # prior probabilities of each class in the training dataset\n",
        "  prior_probab = [(train_size/len(train_data))] * 5;   # '5' is the number of classes in the dataset...\n",
        "\n",
        "  # dividing the dataset class wise...\n",
        "  class_wise=[]\n",
        "  for i in range(0,5):   # '5' is the number of classes in the dataset...\n",
        "    class_wise.append(train_data[(i*train_size) : ((i+1)*train_size)])\n",
        "\n",
        "  # computing the conditional probabilities for each class..\n",
        "  conditional_probab = []\n",
        "  for cls in range(0,5):   # '5' is the number of classes in the dataset...\n",
        "    class_data = class_wise[cls]\n",
        "    yes = [0] * len(effective_features) \n",
        "    no = [0] * len(effective_features)\n",
        "    for i in range(0,len(class_data)):\n",
        "      for j in range(0,len(effective_features)):\n",
        "        if(class_data[i][j] == 1):\n",
        "          yes[j] = yes[j] + 1 \n",
        "        else:\n",
        "          no[j] = no[j] + 1\n",
        "\n",
        "    for k in range(0,len(yes)):\n",
        "      yes[k] = yes[k] / train_size;\n",
        "\n",
        "    for k in range(0,len(no)):\n",
        "      no[k] = no[k] / train_size\n",
        "\n",
        "    conditional_probab.append([yes,no])\n",
        "\n",
        "  # generating the predictions..\n",
        "  predicted_label=[]\n",
        "  for i in range(0,len(test_data)):\n",
        "    test_vec = test_data[i]\n",
        "    class_wise_probab=[]\n",
        "    for cls in range(0,5):\n",
        "      probab_val = prior_probab[cls]\n",
        "      for j in range(0,len(test_vec)):\n",
        "        if(test_vec[j] == 1):\n",
        "          probab_val = probab_val * conditional_probab[cls][0][j]\n",
        "        else:\n",
        "          probab_val = probab_val * conditional_probab[cls][1][j]\n",
        "      class_wise_probab.append(probab_val)\n",
        "    predicted_label.append(class_wise_probab.index(max(class_wise_probab)))\n",
        "\n",
        "  return predicted_label"
      ],
      "metadata": {
        "id": "DrGzMjsKCrfU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-1bcpQnPU-P7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes(k,train_split_ratio):\n",
        "\n",
        "  test_split_ratio  = (1 - train_split_ratio)\n",
        "  test_split_ratio = round(test_split_ratio,2)\n",
        "\n",
        "  path1 = \"/content/drive/MyDrive/IR Assignment 2/Documents/comp.graphics\"\n",
        "  path2 = \"/content/drive/MyDrive/IR Assignment 2/Documents/rec.sport.hockey\"\n",
        "  path3 = \"/content/drive/MyDrive/IR Assignment 2/Documents/sci.med\"\n",
        "  path4 = \"/content/drive/MyDrive/IR Assignment 2/Documents/sci.space\"\n",
        "  path5 = \"/content/drive/MyDrive/IR Assignment 2/Documents/talk.politics.misc\"\n",
        "\n",
        "  dataframe1 = get_dataframe(path1)\n",
        "  dataframe2 = get_dataframe(path2)\n",
        "  dataframe3 = get_dataframe(path3)\n",
        "  dataframe4 = get_dataframe(path4)\n",
        "  dataframe5 = get_dataframe(path5)\n",
        "\n",
        "  #print(dataframe1.shape,dataframe2.shape,dataframe3.shape,dataframe4.shape,dataframe5.shape)\n",
        "\n",
        "  # splitting the dataset into train and test data..\n",
        "  train_1,test_1 = split(dataframe1,train_split_ratio)\n",
        "  train_2,test_2 = split(dataframe2,train_split_ratio)\n",
        "  train_3,test_3 = split(dataframe3,train_split_ratio)\n",
        "  train_4,test_4 = split(dataframe4,train_split_ratio)\n",
        "  train_5,test_5 = split(dataframe5,train_split_ratio)\n",
        "\n",
        "  # getting dictionary of unique words with their TF values for each class in the train data...\n",
        "  graphics_vocab = findTF(path1,train_1)\n",
        "  sport_vocab = findTF(path2,train_2)\n",
        "  scimed_vocab = findTF(path3,train_3)\n",
        "  scispace_vocab = findTF(path4,train_4)\n",
        "  politics_vocab = findTF(path5,train_5)\n",
        "\n",
        "\n",
        "  # TF-ICF values for each class..\n",
        "  graphics_dict = calculate_TFICF(graphics_vocab,sport_vocab,scimed_vocab,scispace_vocab,politics_vocab)\n",
        "  # after sorting\n",
        "  graphics_dict = dict(sorted(graphics_dict.items(), key=lambda item: item[1],reverse=True))\n",
        "\n",
        "  sport_dict = calculate_TFICF(sport_vocab,graphics_vocab,scimed_vocab,scispace_vocab,politics_vocab)\n",
        "  # after sorting\n",
        "  sport_dict = dict(sorted(sport_dict.items(), key=lambda item: item[1],reverse=True))\n",
        "\n",
        "  scimed_dict = calculate_TFICF(scimed_vocab,graphics_vocab,sport_vocab,scispace_vocab,politics_vocab)\n",
        "  # after sorting\n",
        "  scimed_dict = dict(sorted(scimed_dict.items(), key=lambda item: item[1],reverse=True))\n",
        "\n",
        "  scispace_dict = calculate_TFICF(scispace_vocab,graphics_vocab,sport_vocab,scimed_vocab,politics_vocab)\n",
        "  # after sorting\n",
        "  scispace_dict = dict(sorted(scispace_dict.items(), key=lambda item: item[1],reverse=True))\n",
        "\n",
        "  politics_dict = calculate_TFICF(politics_vocab,graphics_vocab,sport_vocab,scimed_vocab,scispace_vocab)\n",
        "  # after sorting\n",
        "  politics_dict = dict(sorted(politics_dict.items(), key=lambda item: item[1],reverse=True))\n",
        "\n",
        "\n",
        "  # getting the union of top k features of each class.. \n",
        "  effective_features = effective_Vocab(graphics_dict,sport_dict,scimed_dict,scispace_dict,politics_dict,k) \n",
        "\n",
        "\n",
        "  # creating list of word occurance of top k features for each document of each class for the training set..\n",
        "  word_occur_train = []\n",
        "  word_occur_train.extend(get_wordOccurance(path1,train_1,effective_features))\n",
        "  word_occur_train.extend(get_wordOccurance(path2,train_2,effective_features))\n",
        "  word_occur_train.extend(get_wordOccurance(path3,train_3,effective_features))\n",
        "  word_occur_train.extend(get_wordOccurance(path4,train_4,effective_features))\n",
        "  word_occur_train.extend(get_wordOccurance(path5,train_5,effective_features))\n",
        "\n",
        "  # creating list of word occurance of top k features for each document of each class for the testing set..\n",
        "  word_occur_test = []\n",
        "  word_occur_test.extend(get_wordOccurance(path1,test_1,effective_features))\n",
        "  word_occur_test.extend(get_wordOccurance(path2,test_2,effective_features))\n",
        "  word_occur_test.extend(get_wordOccurance(path3,test_3,effective_features))\n",
        "  word_occur_test.extend(get_wordOccurance(path4,test_4,effective_features))\n",
        "  word_occur_test.extend(get_wordOccurance(path5,test_5,effective_features))\n",
        "\n",
        "  # creating class mapping dictionary..\n",
        "  class_mapping = {0:'comp.graphics',1:'rec.sport.hockey',2:'sci.med',3:'sci.space',4:'talk.politics.misc'}\n",
        "\n",
        "\n",
        "  # creating the DataFrame class labels for the training set...\n",
        "  class_labels_train = [0] * int(train_split_ratio*1000)\n",
        "  class_labels_train.extend([1] * int(train_split_ratio*1000))\n",
        "  class_labels_train.extend([2] * int(train_split_ratio*1000))\n",
        "  class_labels_train.extend([3] * int(train_split_ratio*1000))\n",
        "  class_labels_train.extend([4] * int(train_split_ratio*1000))\n",
        "  train_y = pd.DataFrame(class_labels_train,columns=['Label'])\n",
        "\n",
        "  # creating the DataFrame class labels for the testing set...\n",
        "  class_labels_test = [0] * int(test_split_ratio*1000)\n",
        "  class_labels_test.extend([1] * int(test_split_ratio*1000))\n",
        "  class_labels_test.extend([2] * int(test_split_ratio*1000))\n",
        "  class_labels_test.extend([3] * int(test_split_ratio*1000))\n",
        "  class_labels_test.extend([4] * int(test_split_ratio*1000))\n",
        "  test_y = pd.DataFrame(class_labels_test,columns=['Label'])\n",
        "\n",
        "  # creating the doclist for the training set...\n",
        "  doclist_train=[]\n",
        "  doclist_train.extend(train_1)\n",
        "  doclist_train.extend(train_2)\n",
        "  doclist_train.extend(train_3)\n",
        "  doclist_train.extend(train_4)\n",
        "  doclist_train.extend(train_5)\n",
        "\n",
        "  # creating the doclist for the test set...\n",
        "  doclist_test=[]\n",
        "  doclist_test.extend(test_1)\n",
        "  doclist_test.extend(test_2)\n",
        "  doclist_test.extend(test_3)\n",
        "  doclist_test.extend(test_4)\n",
        "  doclist_test.extend(test_5)\n",
        "\n",
        "  #print(len(doclist_test),len(doclist_train))\n",
        "\n",
        "  # creating the training dataframe for all the classes using the top k vocabulary obtianed..\n",
        "  train_data = pd.DataFrame(doclist_train,columns=['Documents'])\n",
        "  for word in effective_features:\n",
        "    train_data[word] = 0\n",
        "\n",
        "  # outer loop will be for the total number of documents in the first class..\n",
        "  # inner loop will iterate for the top k features..\n",
        "  for i in range(0,len(doclist_train)):\n",
        "    for j in range(0,len(effective_features)):\n",
        "      train_data[effective_features[j]][i] = word_occur_train[i][j]\n",
        "\n",
        "  # creating the testing dataframe for all the classes using the top k vocabulary obtianed..\n",
        "  test_data = pd.DataFrame(doclist_test,columns=['Documents'])\n",
        "  for word in effective_features:\n",
        "     test_data[word] = 0\n",
        "\n",
        "  # outer loop will be for the total number of documents in the first class..\n",
        "  # inner loop will iterate for the top k features..\n",
        "  for i in range(0,len(doclist_test)):\n",
        "    for j in range(0,len(effective_features)):\n",
        "      test_data[effective_features[j]][i] = word_occur_test[i][j]\n",
        "\n",
        "  train_x = train_data.drop(['Documents'],axis=1)\n",
        "  test_x = test_data.drop(['Documents'],axis=1)\n",
        "\n",
        "  #print(test_x.shape,train_x.shape,test_y.shape,train_y.shape)\n",
        "\n",
        "  # Caliing Naive Bayes Function to get predictions...\n",
        "  predicted = predict_NB(word_occur_train,word_occur_test,effective_features,train_split_ratio)\n",
        "\n",
        "  pred = np.asarray(predicted)\n",
        "  pred = pred.reshape(-1,1)\n",
        "  # Computing Accuracy...\n",
        "  print(\"Accuracy : \",find_accuracy(list(test_y['Label']),predicted)*100)\n",
        "  print()\n",
        "  print(\"Confusion Matrix : \")\n",
        "  print()\n",
        "  # Confusion Matrix...\n",
        "  print(confusion_matrix(test_y,pred))\n",
        "  print()\n",
        "  print(\"Classification Report : \")\n",
        "  print()\n",
        "  # Classification Report...\n",
        "  print(classification_report(test_y,pred))"
      ],
      "metadata": {
        "id": "_lPft1lkU-xR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(35,0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn9Tl3pQbz6f",
        "outputId": "694d3fc5-a466-4aef-cb95-502cb9e6b2c1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  93.0\n",
            "\n",
            "Confusion Matrix : \n",
            "\n",
            "[[200   0   0   0   0]\n",
            " [  5 195   0   0   0]\n",
            " [  3   0 194   3   0]\n",
            " [  6   0  38 152   4]\n",
            " [ 11   0   0   0 189]]\n",
            "\n",
            "Classification Report : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94       200\n",
            "           1       1.00      0.97      0.99       200\n",
            "           2       0.84      0.97      0.90       200\n",
            "           3       0.98      0.76      0.86       200\n",
            "           4       0.98      0.94      0.96       200\n",
            "\n",
            "    accuracy                           0.93      1000\n",
            "   macro avg       0.94      0.93      0.93      1000\n",
            "weighted avg       0.94      0.93      0.93      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(35,0.7)"
      ],
      "metadata": {
        "id": "4TjjYvZiqG0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d622d128-a784-4a85-ee24-49f34a577d4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  93.46666666666667\n",
            "\n",
            "Confusion Matrix : \n",
            "\n",
            "[[290   0   9   0   1]\n",
            " [  5 295   0   0   0]\n",
            " [  7   0 288   2   3]\n",
            " [  4   0  58 234   4]\n",
            " [  4   0   0   1 295]]\n",
            "\n",
            "Classification Report : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95       300\n",
            "           1       1.00      0.98      0.99       300\n",
            "           2       0.81      0.96      0.88       300\n",
            "           3       0.99      0.78      0.87       300\n",
            "           4       0.97      0.98      0.98       300\n",
            "\n",
            "    accuracy                           0.93      1500\n",
            "   macro avg       0.94      0.93      0.93      1500\n",
            "weighted avg       0.94      0.93      0.93      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes(35,0.5)"
      ],
      "metadata": {
        "id": "HgXmzoIhjmHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28585614-e164-49b7-9da8-be81c04dbe33"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  96.56\n",
            "\n",
            "Confusion Matrix : \n",
            "\n",
            "[[487   0   3  10   0]\n",
            " [ 13 487   0   0   0]\n",
            " [ 17   0 483   0   0]\n",
            " [ 17   0   0 474   9]\n",
            " [ 17   0   0   0 483]]\n",
            "\n",
            "Classification Report : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.97      0.93       500\n",
            "           1       1.00      0.97      0.99       500\n",
            "           2       0.99      0.97      0.98       500\n",
            "           3       0.98      0.95      0.96       500\n",
            "           4       0.98      0.97      0.97       500\n",
            "\n",
            "    accuracy                           0.97      2500\n",
            "   macro avg       0.97      0.97      0.97      2500\n",
            "weighted avg       0.97      0.97      0.97      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9uE4_I4wRZvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}